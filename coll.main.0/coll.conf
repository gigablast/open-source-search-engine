# All <, >, " and # characters that are values for a field contained herein
# must be represented as &lt;, &gt;, &#34; and &#035; respectively.

# Controls just the spiders for this collection.
<spideringEnabled>1</>

# What is the maximum number of web pages the spider is allowed to download
# simultaneously PER HOST for THIS collection?
<maxSpiders>100</>

# make each spider wait this many milliseconds before getting the ip and
# downloading the page.
<spiderDelayInMilliseconds>0</>

# If this is true Gigablast will respect the robots.txt convention.
<useRobotstxt>1</>

# How many second to cache a robots.txt file for. 86400 is 1 day. 0 means
# Gigablast will not read from the cache at all and will download the
# robots.txt before every page if robots.txt use is enabled above. However, if
# this is 0 then Gigablast will still store robots.txt files into the cache.
<maxRobotstxtCacheAge>86400</>

# Do a tight merge on posdb and titledb at this time every day. This is
# expressed in MINUTES past midnight UTC. UTC is 5 hours ahead of EST and 7
# hours ahead of MST. Leave this as -1 to NOT perform a daily merge. To merge
# at midnight EST use 60*5=300 and midnight MST use 60*7=420.
<dailyMergeTime>-1</>

# Comma separated list of days to merge on. Use 0 for Sunday, 1 for Monday,
# ... 6 for Saturday. Leaving this parmaeter empty or without any numbers will
# make the daily merge happen every day
<dailyMergeDays><![CDATA[0]]></>

# When the daily merge was last kicked off. Expressed in UTC in seconds since
# the epoch.
<dailyMergeLastStarted>-1</>

# If this is true, users will have to pass a simple Turing test to add a url.
# This prevents automated url submission.
<turingTestEnabled>0</>

# Maximum number of urls that can be submitted via the addurl interface, per
# IP domain, per 24 hour period. A value less than or equal to zero implies no
# limit.
<maxAddUrls>0</>

# When the spider round started
<spiderRoundStartTime>0</>

# The spider round number.
<spiderRoundNum>0</>

# When enabled, the spider will discard web pages which are identical to other
# web pages that are already in the index. However, root urls, urls that have
# no path, are never discarded. It most likely has to hit disk to do these
# checks so it does cause some slow down. Only use it if you need it.
<dedupingEnabled>0</>

# When enabled, the spider will discard web pages which, when a www is
# prepended to the page's url, result in a url already in the index.
<dedupingEnabledForWww>1</>

# Detect and do not index pages which have a 200 status code, but are likely
# to be error pages.
<detectCustomErrorPages>1</>

# Should pages be removed from the index if they are no longer accessible on
# the web?
<delete404s>1</>

# If this is true, the spider, when a url redirects to a "simpler" url, will
# add that simpler url into the spider queue and abandon the spidering of the
# current url.
<useSimplifiedRedirects>1</>

# If this is true, the spider, when updating a web page that is already in the
# index, will not even download the whole page if it hasn't been updated since
# the last time Gigablast spidered it. This is primarily a bandwidth saving
# feature. It relies on the remote webserver's returned Last-Modified-Since
# field being accurate.
<useIfModifiedSince>0</>

# If this is true, do not allow spammy inlinks to vote. This check is too
# aggressive for some collections, i.e.  it does not allow pages with cgi in
# their urls to vote.
<doLinkSpamChecking>1</>

# If this is true Gigablast will only allow one vote per the top 2 significant
# bytes of the IP address. Otherwise, multiple pages from the same top IP can
# contribute to the link text and link-based quality ratings of a particular
# URL. Furthermore, no votes will be accepted from IPs that have the same top
# 2 significant bytes as the IP of the page being indexed.
<restrictLinkVotingByIp>1</>

# How often should Gigablast recompute the link info for a url. Also applies
# to getting the quality of a site or root url, which is based on the link
# info. In days. Can use decimals. 0 means to update the link info every time
# the url's content is re-indexed. If the content is not reindexed because it
# is unchanged then the link info will not be updated. When getting the link
# info or quality of the root url from an external cluster, Gigablast will
# tell the external cluster to recompute it if its age is this or higher.
<updateLinkInfoFrequency>60.000000</>

# If this is eabled the spider will not allow any docs which are determined to
# be serps.
<doSerpDetection>1</>

# If this is false then the filter will not be used on html or text pages.
<applyFilterToTextPages>0</>

# Program to spawn to filter all HTTP replies the spider receives. Leave blank
# for none.
<filterName><![CDATA[]]></>

# Kill filter shell after this many seconds. Assume it stalled permanently.
<filterTimeout>40</>

# Retrieve pages from the proxy at this IP address.
<proxyIp>0.0.0.0</>

# Retrieve pages from the proxy on this port.
<proxyPort>0</>

# Index the body of the documents so you can search it. Required for searching
# that. You wil pretty much always want to keep this enabled.
<indexBody>1</>

# Send every spidered url to this diffbot.com by appending a &url=<url> to it
# before trinyg to downloading it. We expect get get back a JSON reply which
# we index. You will need to supply your token to this as well.
<diffbotApiUrl><![CDATA[]]></>

# Get scoring information for each result so you can see how each result is
# scored? You must explicitly request this using &scores=1 for the XML feed
# because it is not included by default.
<getDocidScoringInfo>1</>

# Query expansion will include word stems and synonyms in its search results.
<doQueryExpansion>1</>

# What is the limit to the total number of returned search results.
<maxSearchResults>1000</>

# What is the limit to the total number of returned search results per query?
<maxSearchResultsPerQuery>100</>

# What is the maximum number of characters allowed in titles displayed in the
# search results?
<maxTitleLen>80</>

# Should search results be site clustered by default?
<siteClusterByDefault>1</>

# Hide all clustered results instead of displaying two results from each site.
<hideAllClusteredResults>0</>

# Should duplicate search results be removed by default?
<dedupResultsByDefault>1</>

# Should we dedup URLs with case insensitivity? This is mainly to correct
# duplicate wiki pages.
<dedupURLs>0</>

# If document summary is this percent similar to a document summary above it,
# then remove it from the search results. 100 means only to remove if exactly
# the same. 0 means no summary deduping.
<percentSimilarDedupSummary>90</>

# Sets the number of lines to generate for summary deduping. This is to help
# the deduping process not thorw out valid summaries when normally displayed
# summaries are smaller values. Requires percent similar dedup summary to be
# enabled.
<numberOfLinesToUseInSummaryToDedup>4</>

# Default language to use for ranking results. Value should be any language
# abbreviation, for example "en" for English.
<sortLanguagePreference><![CDATA[en]]></>

# Default country to use for ranking results. Value should be any country code
# abbreviation, for example "us" for United States.
<sortCountryPreference><![CDATA[us]]></>

# What is the maximum number of characters displayed in a summary for a search
# result?
<maxSummaryLen>512</>

# What is the maximum number of excerpts displayed in the summary of a search
# result?
<maxSummaryExcerpts>4</>

# What is the maximum number of characters allowed per summary excerpt?
<maxSummaryExcerptLength>300</>

# What is the default number of summary excerpts displayed per search result?
<defaultNumberOfSummaryExcerpts>3</>

# <br> tags are inserted to keep the number of chars in the summary per line
# at or below this width. Strings without spaces that exceed this width are
# not split.
<maxSummaryLineWidth>80</>

# Truncating this will miss out on good summaries, but performance will
# increase.
<bytesOfDocToScanForSummaryGeneration>70000</>

# Front html tag used for highlightig query terms in the summaries displated
# in the search results.
<frontHighlightTag><![CDATA[&lt;b style=&#34;color:black;background-color:&#035;ffff66&#34;&gt;]]></>

# Front html tag used for highlightig query terms in the summaries displated
# in the search results.
<backHighlightTag><![CDATA[&lt;/b&gt;]]></>

# How many search results should we scan for related topics (gigabits) per
# query?
<docsToScanForTopics>300</>

# Should Gigablast only get one document per IP domain and per domain for
# topic (gigabit) generation?
<ipRestrictionForTopics>0</>

# Should Gigablast remove overlapping topics (gigabits)?
<removeOverlappingTopics>1</>

# What is the number of related topics (gigabits) displayed per query? Set to
# 0 to save CPU time.
<numberOfRelatedTopics>11</>

# Related topics (gigabits) with scores below this will be excluded. Scores
# range from 0% to over 100%.
<minTopicsScore>5</>

# How many documents must contain the topic (gigabit) for it to be displayed.
<minTopicDocCount>2</>

# If a document is this percent similar to another document with a higher
# score, then it will not contribute to the topic (gigabit) generation.
<dedupDocPercentForTopics>80</>

# Maximum number of words a topic (gigabit) can have. Affects raw feeds, too.
<maxWordsPerTopic>6</>

# Max chars to sample from each doc for topics (gigabits).
<topicMaxSampleSize>4096</>

# If enabled, results in dmoz will display their categories on the results
# page.
<displayDmozCategoriesInResults>1</>

# If enabled, results in dmoz will display their indirect categories on the
# results page.
<displayIndirectDmozCategoriesInResults>0</>

# If enabled, a link will appear next to each category on each result allowing
# the user to perform their query on that entire category.
<displaySearchCategoryLinkToQueryCategoryOfResult>0</>

# Yes to use DMOZ given title when a page is untitled but is in DMOZ.
<useDmozForUntitled>1</>

# Yes to always show DMOZ summaries with search results that are in DMOZ.
<showDmozSummaries>1</>

# Yes to display the Adult category in the Top category
<showAdultCategoryOnTop>0</>

# Before downloading the contents of a URL, Gigablast first chains down this
# list of expressions</a>, starting with expression #0.  The first expression
# it matches is the ONE AND ONLY matching row for that url. It then uses the
# respider frequency, spider priority, etc. on the MATCHING ROW when spidering
# that URL. If you specify the <i>expression</i> as <i><b>default</b></i> then
# that MATCHES ALL URLs. URLs with high spider priorities take spidering
# precedence over URLs with lower spider priorities. The respider frequency
# dictates how often a URL will be respidered. See the help table below for
# examples of all the supported expressions. Use the <i>&&</i> operator to
# string multiple expressions together in the same expression text box. A
# <i>spider priority</i> of <i>FILTERED</i> or <i>BANNED</i> will cause the
# URL to not be spidered, or if it has already been indexed, it will be
# deleted when it is respidered.<br><br>
<filterExpression><![CDATA[isdocidbased]]></>
<filterExpression><![CDATA[ismedia]]></>
<filterExpression><![CDATA[errorcount&gt;=3 &amp;&amp; hastmperror]]></>
<filterExpression><![CDATA[errorcount&gt;=1 &amp;&amp; hastmperror]]></>
<filterExpression><![CDATA[isaddurl]]></>
<filterExpression><![CDATA[hopcount==0 &amp;&amp; iswww &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==0 &amp;&amp; iswww]]></>
<filterExpression><![CDATA[hopcount==0 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==0]]></>
<filterExpression><![CDATA[hopcount==1 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==1]]></>
<filterExpression><![CDATA[hopcount==2 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==2]]></>
<filterExpression><![CDATA[hopcount&gt;=3 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount&gt;=3]]></>
<filterExpression><![CDATA[isnew]]></>
<filterExpression><![CDATA[default]]></>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<harvestLinks>1</>
<filterFrequency>0.000000</>
<filterFrequency>0.000000</>
<filterFrequency>1.000000</>
<filterFrequency>1.000000</>
<filterFrequency>1.000000</>
<filterFrequency>7.000000</>
<filterFrequency>7.000000</>
<filterFrequency>7.000000</>
<filterFrequency>10.000000</>
<filterFrequency>20.000000</>
<filterFrequency>20.000000</>
<filterFrequency>40.000000</>
<filterFrequency>40.000000</>
<filterFrequency>60.000000</>
<filterFrequency>60.000000</>
<filterFrequency>30.000000</>
<filterFrequency>30.000000</>

# Do not allow more than this many outstanding spiders for all urls in this
# priority.
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>4</>
<maxSpidersPerRule>2</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>2</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>

# Allow this many spiders per IP.
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>

# Wait at least this long before downloading urls from the same IP address.
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<filterPriority>80</>
<filterPriority>-3</>
<filterPriority>3</>
<filterPriority>45</>
<filterPriority>85</>
<filterPriority>50</>
<filterPriority>48</>
<filterPriority>49</>
<filterPriority>47</>
<filterPriority>40</>
<filterPriority>39</>
<filterPriority>30</>
<filterPriority>29</>
<filterPriority>20</>
<filterPriority>19</>
<filterPriority>1</>
<filterPriority>0</>
